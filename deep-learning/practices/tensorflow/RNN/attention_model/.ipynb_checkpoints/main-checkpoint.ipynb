{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation Using Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Encoder` encodes the complete information of the source sequence into a single real-valued vector , also known\n",
    "as the context vector, which is passed to the `Decoder` to produce an output sequence , which is the target language like spanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Contex` vector has the responsibility to summarize the entire input sequence into a single vector, which is inefficient , so we use `Attention` mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of `Attention` mechanism is to avoid attempting to learning a single vector representation for each sentence;instead, it pays attention to the specific input vectors of the input sequence based on the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Read the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Clean and Preprocess the source and target sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = unicode_to_ascii(sentence.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "\n",
    "  sentence = sentence.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  sentence = '<start> ' + sentence + ' <end>'\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> can you do it in one second ? <end>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(\"Can you do it in one second?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the source and target sentences to have  word pairs in format `[ENGLISH, SPANISH]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Tokenize the source and target sentences\n",
    "\n",
    "We need to vectorize the text corpus where the text is conveted into a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  inp_lang, targ_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Create training and test dataset\n",
    "\n",
    "split the dataset into  a test and train. 80% of data is used for training and 20% for testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset is big, we want to create the dataset in memory to be efficient. We will use `tf.data.Dataset.from_tensor_slices()` method to get slices of array in the form of an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the batch size\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "\n",
    "# create data in memory\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "\n",
    "# shuffles the data in batch\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 11)\n",
      "(64, 16)\n"
     ]
    }
   ],
   "source": [
    "# Create an iterator for enumerating the elements of the dataset\n",
    "source_batch, target_batch = next(iter(dataset))\n",
    "print(source_batch.shape)\n",
    "print(target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "20 ----> that\n",
      "164 ----> wasn\n",
      "12 ----> t\n",
      "69 ----> good\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "143 ----> podria\n",
      "6413 ----> mejorarse\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[1])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "source_vocab_size = len(inp_lang.word_index) + 1\n",
    "target_vocab_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Create the `Encoder`\n",
    "\n",
    "The `encoder` takes the input as the source tokens, passes them to an `embedding` layer for the dense representation of the vector, which is then passed to `GRU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `return_sequences` and `return_state` as True for the GRU. When the `return_sequences` set to true, then it returns the entire sequences of outputs from all the units in the encoder.\n",
    "When `return_sequences` is set to False, then we only return the hidden state of the last encoder unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return internal state of `GRU`, we set the `return_state` to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(encoder_units, return_sequences = True, return_state = True, recurrent_initializer = 'glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        # pass the input x to the embedding\n",
    "        x = self.embedding(x)\n",
    "        # pass the embedding  and the hidden state to the GRU\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(source_vocab_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_hidden = encoder.initialize_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output, sample_hidden = encoder(source_batch, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch_size, sequence_length, units) (64, 11, 1024)\n",
      "Encoder hidden state shape: (batch_size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "print('Encoder output shape: (batch_size, sequence_length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder hidden state shape: (batch_size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Create the Bahdanau Attention layer\n",
    "\n",
    "attention layer consist of\n",
    "-> Alignment score\n",
    "-> Attention weights\n",
    "-> Context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "      def __init__(self, units):\n",
    "          super(Attention, self).__init__()\n",
    "          self.W1 = tf.keras.layers.Dense(units) # encoder output\n",
    "          self.W2 = tf.keras.layers.Dense(units) # decoder hidden\n",
    "          self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "      def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1) #Returns a tensor with an additional dimension inserted at index axis.\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = Attention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Create the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder has an `embedding` layer, a `GRU` layer, and a fully connected layer\n",
    "\n",
    "To predict the target word Decoder uses\n",
    "##### 1) Context vector: the sum of attention weights and encoder output\n",
    "##### 2) Decoder output from the previous time step\n",
    "##### 3) Previous decoder hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(decoder_units, return_sequences=True, return_state=True, recurrent_initializer = 'glorot_uniform')\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # attention\n",
    "        self.attention = Attention(self.decoder_units)\n",
    "        \n",
    "    def call(self, x, hidden, encoder_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, encoder_output)\n",
    "        \n",
    "        # pass output sequence through input layers\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # concatenate context_vector and embedding for output sequence\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # pass the output through output layer\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 9414)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(target_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "        # create encoder\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        # first input to decode is start_\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        print(dec_input)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "          # passing enc_output to the decoder\n",
    "          predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "          \n",
    "          # calculate loss based on prediction\n",
    "          loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims:0\", shape=(64, 1), dtype=int32)\n",
      "Tensor(\"ExpandDims:0\", shape=(64, 1), dtype=int32)\n",
      "Epoch 1 Batch 0 Loss 3.2613\n",
      "Epoch 1 Batch 100 Loss 1.7069\n",
      "Epoch 1 Batch 200 Loss 1.4418\n",
      "Epoch 1 Batch 300 Loss 1.2672\n",
      "Epoch 1 Loss 1.5599\n",
      "Time taken for 1 epoch 1252.0504822731018 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  \n",
    "  # train the model using data in batches\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f35d46a6e10>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i am a boy . <end>\n",
      "Predicted translation: no me gusta el . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'i am a boy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> slowly . <end>\n",
      "Predicted translation: somos . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'slowly.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
